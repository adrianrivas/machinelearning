{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2eab6e4-eb9f-43cc-883d-2c2d07b8db4f",
      "metadata": {
        "id": "d2eab6e4-eb9f-43cc-883d-2c2d07b8db4f",
        "tags": []
      },
      "source": [
        "Latent Dirichlet Allocation\n",
        "==="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d51828b9-1927-46ca-99b7-f0457bf1fdcd",
      "metadata": {
        "id": "d51828b9-1927-46ca-99b7-f0457bf1fdcd"
      },
      "source": [
        "Preparación\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86a3a063-4324-4f75-af9f-581c698abc5f",
      "metadata": {
        "id": "86a3a063-4324-4f75-af9f-581c698abc5f",
        "outputId": "c03290ae-8a5a-41e7-cfc8-53b8851e5da2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "scopus = pd.read_csv(\"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\")\n",
        "\n",
        "abstract = scopus['Abstract']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6889572-587b-4c6f-a60e-88baae0d2603",
      "metadata": {
        "id": "e6889572-587b-4c6f-a60e-88baae0d2603"
      },
      "source": [
        "Descripción del problema\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b735e5-e97a-4492-b216-70f3e59da6e5",
      "metadata": {
        "id": "11b735e5-e97a-4492-b216-70f3e59da6e5"
      },
      "source": [
        "Uno de los principales problemas abordados en minería de texto consiste en la extracción de los temas o tópicos a los que pertenece documento. Por ejemplo, una noticia podría pertener simultáneamente a los temas de religión y economía (el escándalo por el manejo de fondos del Vaticano). Cuando se tiene un conjunto de documentos, se desea extraer los tópicos subyacentes sobre los que tratan los documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e283e75d-d9f0-438a-9e32-4122777d5e50",
      "metadata": {
        "id": "e283e75d-d9f0-438a-9e32-4122777d5e50"
      },
      "source": [
        "Scikit-learn contiene una implementación de la metodología Latent Dirichlet Allocation, la cual permite extraer los tópicos de un conjunto de documentos. Véase https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b577cb58-f6d9-4e98-9310-0a56fb931840",
      "metadata": {
        "id": "b577cb58-f6d9-4e98-9310-0a56fb931840"
      },
      "source": [
        "Utilice esta metodología para extraer los tópicos subyacentes en los abstracts de los artículos. Tenga en cuenta que:\n",
        "\n",
        "1. Debe establecer como obtener el número apropiado de tópicos a obtener.\n",
        "\n",
        "2. Debe eliminar las stop-words.\n",
        "\n",
        "3. En T-Lab sugieren reducir las palabras a sustantivos, adjetivos, verbos y adverbios únicamente. Cómo podría realizar esto en su código=?\n",
        "\n",
        "4. Cómo podría verificar si la cantidad de temas es apropiada desde el punto de vista de su contenido (las palabras que contiene y los temas que trata)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9bfecf4d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       Mobility is one of the fundamental requirement...\n",
              "1       The recent rise of the political extremism in ...\n",
              "2       The power of the press to shape the informatio...\n",
              "3       Identifying influential nodes in a network is ...\n",
              "4       To complement traditional dietary surveys, whi...\n",
              "                              ...                        \n",
              "1897    In this article, we intend to show how useful ...\n",
              "1898    In recent geographical information science lit...\n",
              "1899    The fact that many decisions need a combinatio...\n",
              "1900    The report from Woolmark Business Intelligence...\n",
              "1901    Changing consumer lifestyles and increased tim...\n",
              "Name: Abstract, Length: 1902, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Se muestran los registros almacenados en la columna Abstracts del dataset\n",
        "\n",
        "abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3f7c8639",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Se importan las librerias a utilizar en el laboratorio\n",
        "\n",
        "import warnings\n",
        "# init\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "#np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "dae0ba56",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Función para generar los lemmas y los stem correspondientes, \n",
        "#asimismo eliminar las stopwords y las palabras que no tengan más de 3 letras\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocesing(row):\n",
        "\n",
        "    text_stemmed = []\n",
        "    text_splited = []\n",
        "    new_text = []\n",
        "\n",
        "    row = row.replace('.', '')\n",
        "    row = row.replace(',', '')\n",
        "    text_splited = row.split(' ')\n",
        "\n",
        "    for a in text_splited:\n",
        "        text_stemmed.append(stemmer.stem(WordNetLemmatizer().lemmatize(a, pos='v')))\n",
        "\n",
        "    text_stemmed\n",
        "\n",
        "    for a in range(len(text_stemmed)):\n",
        "        if text_stemmed[a] not in gensim.parsing.preprocessing.STOPWORDS and len(text_stemmed[a]) > 3:\n",
        "            new_text.append(text_stemmed[a])\n",
        "    \n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "abdb25b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Abstract_preprocesed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mobility is one of the fundamental requirement...</td>\n",
              "      <td>[mobil, fundament, requir, human, life, signif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The recent rise of the political extremism in ...</td>\n",
              "      <td>[recent, rise, polit, extrem, western, countri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The power of the press to shape the informatio...</td>\n",
              "      <td>[power, press, shape, inform, landscap, popul,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Identifying influential nodes in a network is ...</td>\n",
              "      <td>[identifi, influenti, network, fundament, issu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To complement traditional dietary surveys, whi...</td>\n",
              "      <td>[complement, tradit, dietari, survey, costli, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1897</th>\n",
              "      <td>In this article, we intend to show how useful ...</td>\n",
              "      <td>[articl, intend, exploratori, spatial, data, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1898</th>\n",
              "      <td>In recent geographical information science lit...</td>\n",
              "      <td>[recent, geograph, inform, scienc, literatur, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1899</th>\n",
              "      <td>The fact that many decisions need a combinatio...</td>\n",
              "      <td>[fact, mani, decis, need, combin, inform, sour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1900</th>\n",
              "      <td>The report from Woolmark Business Intelligence...</td>\n",
              "      <td>[report, woolmark, busi, intellig, consid, rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>Changing consumer lifestyles and increased tim...</td>\n",
              "      <td>[chang, consum, lifestyl, increas, time, press...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1902 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Abstract  \\\n",
              "0     Mobility is one of the fundamental requirement...   \n",
              "1     The recent rise of the political extremism in ...   \n",
              "2     The power of the press to shape the informatio...   \n",
              "3     Identifying influential nodes in a network is ...   \n",
              "4     To complement traditional dietary surveys, whi...   \n",
              "...                                                 ...   \n",
              "1897  In this article, we intend to show how useful ...   \n",
              "1898  In recent geographical information science lit...   \n",
              "1899  The fact that many decisions need a combinatio...   \n",
              "1900  The report from Woolmark Business Intelligence...   \n",
              "1901  Changing consumer lifestyles and increased tim...   \n",
              "\n",
              "                                   Abstract_preprocesed  \n",
              "0     [mobil, fundament, requir, human, life, signif...  \n",
              "1     [recent, rise, polit, extrem, western, countri...  \n",
              "2     [power, press, shape, inform, landscap, popul,...  \n",
              "3     [identifi, influenti, network, fundament, issu...  \n",
              "4     [complement, tradit, dietari, survey, costli, ...  \n",
              "...                                                 ...  \n",
              "1897  [articl, intend, exploratori, spatial, data, a...  \n",
              "1898  [recent, geograph, inform, scienc, literatur, ...  \n",
              "1899  [fact, mani, decis, need, combin, inform, sour...  \n",
              "1900  [report, woolmark, busi, intellig, consid, rec...  \n",
              "1901  [chang, consum, lifestyl, increas, time, press...  \n",
              "\n",
              "[1902 rows x 2 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Generar un dataframe con la columna de abstracts más \n",
        "#la columna de los abstracts preprocesados por nuestra función de preprocesamiento creada anteriormente.\n",
        "\n",
        "new_df=pd.DataFrame()\n",
        "\n",
        "new_df['Abstract'] = abstract\n",
        "new_df['Abstract_preprocesed'] = new_df['Abstract'].map(preprocesing)\n",
        "\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6a5f42ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 2019\n",
            "1 abil\n",
            "2 adapt\n",
            "3 appli\n",
            "4 assess\n",
            "5 author(s)\n",
            "6 chang\n",
            "7 climat\n",
            "8 concept\n",
            "9 data\n",
            "10 defin\n"
          ]
        }
      ],
      "source": [
        "abstract_preprocesed = new_df['Abstract_preprocesed']\n",
        "\n",
        "# Se crea un diccionario para computar el número de apariciones de cada término\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(abstract_preprocesed)\n",
        "\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "45f82b00",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se filtran del diccionario las palabras que no se repiten en más de 15 registros y se dejan los 100000 términos más frecuentes\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "\n",
        "#Almacena el término y su frecuencia\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in abstract_preprocesed]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ef46f7eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.059093572623355545),\n",
            " (1, 0.0778919649050463),\n",
            " (2, 0.073195683314149),\n",
            " (3, 0.04423026501499936),\n",
            " (4, 0.05879918995997059),\n",
            " (5, 0.028534959984836576),\n",
            " (6, 0.10880116465250018),\n",
            " (7, 0.09726679319184532),\n",
            " (8, 0.0638259051392301),\n",
            " (9, 0.06589707868189897),\n",
            " (10, 0.05954203214876568),\n",
            " (11, 0.030386373441651435),\n",
            " (12, 0.22636094495787815),\n",
            " (13, 0.062424520789007495),\n",
            " (14, 0.09726679319184532),\n",
            " (15, 0.44571846847757157),\n",
            " (16, 0.4574043030158056),\n",
            " (17, 0.0662936478109321),\n",
            " (18, 0.0829235559884092),\n",
            " (19, 0.0877824950569692),\n",
            " (20, 0.0579371132887808),\n",
            " (21, 0.1750928394428846),\n",
            " (22, 0.17772638900405616),\n",
            " (23, 0.044713339443339405),\n",
            " (24, 0.07139865966285804),\n",
            " (25, 0.05724164760701321),\n",
            " (26, 0.0829235559884092),\n",
            " (27, 0.05939161230771521),\n",
            " (28, 0.09265142866266941),\n",
            " (29, 0.04047503315619716),\n",
            " (30, 0.08537369278140047),\n",
            " (31, 0.07090901234010802),\n",
            " (32, 0.060683308325971165),\n",
            " (33, 0.07957040339168775),\n",
            " (34, 0.3509639172486234),\n",
            " (35, 0.2694767744748783),\n",
            " (36, 0.058508375659209605),\n",
            " (37, 0.0945218730825234),\n",
            " (38, 0.08137515514291928),\n",
            " (39, 0.07346375186899194),\n",
            " (40, 0.10783663424469085),\n",
            " (41, 0.07373484951115135),\n",
            " (42, 0.0662936478109321),\n",
            " (43, 0.032150594807472295),\n",
            " (44, 0.06094001315320512),\n",
            " (45, 0.17002224606180294),\n",
            " (46, 0.04683790246241169),\n",
            " (47, 0.06904462724243755),\n",
            " (48, 0.0987667744728523),\n",
            " (49, 0.053918317122345426),\n",
            " (50, 0.0907459034790369),\n",
            " (51, 0.10490709882361328),\n",
            " (52, 0.05440058232625009),\n",
            " (53, 0.06752427068542202),\n",
            " (54, 0.06512307975142383),\n",
            " (55, 0.027560536270608123),\n",
            " (56, 0.055017374291181975),\n",
            " (57, 0.06773565430885627)]\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "#Se analiza qué tan común o poco común es una palabra entre el bow_corpus. \n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "baa03779",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se aplica el algoritmo Latent Dirichlet Allocation para determinar los topicos\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "baed6f36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.036*\"model\" + 0.011*\"develop\" + 0.009*\"algorithm\" + 0.009*\"paper\" + 0.009*\"manag\" + 0.008*\"propos\" + 0.007*\"process\" + 0.007*\"result\" + 0.006*\"present\" + 0.006*\"studi\"\n",
            "Topic: 1 \n",
            "Words: 0.014*\"network\" + 0.012*\"propos\" + 0.010*\"urban\" + 0.008*\"result\" + 0.008*\"cluster\" + 0.008*\"model\" + 0.008*\"approach\" + 0.008*\"effect\" + 0.007*\"method\" + 0.007*\"studi\"\n",
            "Topic: 2 \n",
            "Words: 0.018*\"model\" + 0.010*\"product\" + 0.009*\"qualiti\" + 0.008*\"inform\" + 0.008*\"paper\" + 0.008*\"research\" + 0.008*\"method\" + 0.007*\"develop\" + 0.007*\"differ\" + 0.007*\"measur\"\n",
            "Topic: 3 \n",
            "Words: 0.013*\"social\" + 0.011*\"predict\" + 0.009*\"result\" + 0.008*\"differ\" + 0.008*\"approach\" + 0.007*\"model\" + 0.007*\"studi\" + 0.007*\"inform\" + 0.007*\"method\" + 0.007*\"propos\"\n",
            "Topic: 4 \n",
            "Words: 0.012*\"analysi\" + 0.010*\"paper\" + 0.009*\"perform\" + 0.008*\"studi\" + 0.007*\"model\" + 0.007*\"inform\" + 0.007*\"market\" + 0.007*\"approach\" + 0.006*\"research\" + 0.006*\"develop\"\n",
            "Topic: 5 \n",
            "Words: 0.010*\"network\" + 0.009*\"studi\" + 0.009*\"develop\" + 0.008*\"provid\" + 0.008*\"differ\" + 0.007*\"time\" + 0.007*\"model\" + 0.007*\"base\" + 0.006*\"popul\" + 0.006*\"paper\"\n",
            "Topic: 6 \n",
            "Words: 0.028*\"research\" + 0.011*\"servic\" + 0.010*\"manag\" + 0.009*\"technolog\" + 0.009*\"develop\" + 0.008*\"scienc\" + 0.008*\"share\" + 0.008*\"inform\" + 0.007*\"materi\" + 0.007*\"support\"\n",
            "Topic: 7 \n",
            "Words: 0.017*\"method\" + 0.016*\"algorithm\" + 0.011*\"propos\" + 0.010*\"cluster\" + 0.010*\"model\" + 0.009*\"studi\" + 0.009*\"process\" + 0.008*\"result\" + 0.008*\"paper\" + 0.008*\"dataset\"\n",
            "Topic: 8 \n",
            "Words: 0.012*\"result\" + 0.011*\"method\" + 0.011*\"algorithm\" + 0.011*\"network\" + 0.011*\"time\" + 0.010*\"propos\" + 0.008*\"paper\" + 0.008*\"base\" + 0.008*\"model\" + 0.007*\"perform\"\n",
            "Topic: 9 \n",
            "Words: 0.017*\"inform\" + 0.012*\"model\" + 0.011*\"research\" + 0.009*\"process\" + 0.009*\"propos\" + 0.009*\"scienc\" + 0.008*\"base\" + 0.008*\"paper\" + 0.007*\"manag\" + 0.007*\"develop\"\n"
          ]
        }
      ],
      "source": [
        "#Se muestran los tópicos y las palabras relacionadas a cada uno\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6acbb568",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.684433102607727\t \n",
            "Topic: 0.017*\"method\" + 0.016*\"algorithm\" + 0.011*\"propos\" + 0.010*\"cluster\" + 0.010*\"model\" + 0.009*\"studi\" + 0.009*\"process\" + 0.008*\"result\" + 0.008*\"paper\" + 0.008*\"dataset\"\n",
            "\n",
            "Score: 0.3034411072731018\t \n",
            "Topic: 0.012*\"result\" + 0.011*\"method\" + 0.011*\"algorithm\" + 0.011*\"network\" + 0.011*\"time\" + 0.010*\"propos\" + 0.008*\"paper\" + 0.008*\"base\" + 0.008*\"model\" + 0.007*\"perform\"\n"
          ]
        }
      ],
      "source": [
        "#Se prueba el modelo con un registro aleatorio del dataset en favor de evaluar el tópico al cual pertenece\n",
        "for index, score in sorted(lda_model[bow_corpus[315]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "08-003-latent_dirichlet_allocation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
